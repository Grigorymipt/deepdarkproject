{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "torch.Size([3, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\Desktop\\Digital_MIPT\\code\\sbert_code\\.conda\\Lib\\site-packages\\transformers\\modeling_utils.py:4779: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\Desktop\\Digital_MIPT\\code\\sbert_code\\.conda\\Lib\\site-packages\\torch\\onnx\\utils.py:782: UserWarning: no signature found for builtin <built-in method __call__ of PyCapsule object at 0x00000188564E7990>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been successfully converted to ONNX and saved at sbert_onnx_2.onnx\n"
     ]
    }
   ],
   "source": [
    "# REQUIRE torch, onnx AND onnxruntime\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "class BertWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(BertWrapper, self).__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, mega_tensor):\n",
    "        output = self.model(input_ids=torch.split(mega_tensor, 1, dim=0)[0],\n",
    "                             token_type_ids=torch.split(mega_tensor, 1, dim=0)[1],\n",
    "                               attention_mask=torch.split(mega_tensor, 1, dim=0)[2])\n",
    "        return output.last_hidden_state\n",
    "\n",
    "\n",
    "def get_dummy_input():\n",
    "    #Tokenize sentences\n",
    "    dummy_input = {'input_ids':torch.ones(512, dtype=int), 'token_type_ids':torch.ones(512, dtype=int), 'attention_mask':torch.ones(512, dtype=int)}\n",
    "    print(torch.ones(512))\n",
    "    output = torch.stack((dummy_input['input_ids'], dummy_input['token_type_ids'], dummy_input['attention_mask']), dim=0)\n",
    "    return output\n",
    "\n",
    "''' THE FIRST PART (GETTING WRAPPED SBERT) '''\n",
    "dummy_input = get_dummy_input()\n",
    "print(dummy_input.shape)\n",
    "model = AutoModel.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "wrapped_model = BertWrapper(model)\n",
    "# Use TorchScript scripting to convert the model\n",
    "scripted_model = torch.jit.trace(wrapped_model, dummy_input)\n",
    "\n",
    "''' THE SECOND PART (CONVERTING WRAPPED SBERT INTO ONNX) '''\n",
    "# 1. Load your PyTorch model (Assume it's a simple CNN here)\n",
    "# Replace 'model_class' with your model's class and 'model_weights.pt' with your .pt file\n",
    "model.eval()  # 2. Set the model to evaluation mode\n",
    "dummy_input = get_dummy_input()\n",
    "\n",
    "# 4. Define the export parameters\n",
    "onnx_file_path = \"sbert_onnx_2.onnx\"\n",
    "\n",
    "\n",
    "torch.onnx.export(\n",
    "    scripted_model,                        # Your SBERT model\n",
    "    dummy_input,                  # Tuple of input_ids and attention_mask\n",
    "    onnx_file_path,               # Save path for ONNX model\n",
    "    export_params=True,           # Store trained parameters\n",
    "    opset_version=14,             # Use Opset version 14 to support scaled_dot_product_attention\n",
    "    do_constant_folding=True,     # Optimize constants\n",
    "    input_names=['input_ids'],  # Input names\n",
    "    output_names=['output']      # Output name\n",
    ")\n",
    "\n",
    "print(f\"Model has been successfully converted to ONNX and saved at {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(sentences=['Привет! Как твои дела?']):\n",
    "    #Load AutoModel from huggingface model repository\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "\n",
    "    #Tokenize sentences\n",
    "    dummy_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
    "    output = torch.cat((dummy_input['input_ids'], dummy_input['token_type_ids'], dummy_input['attention_mask']), dim=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output \n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Name: input_ids\n",
      "Input Shape: [3, 8]\n",
      "Input Type: tensor(int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_2032\\795341922.py:17: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  input_data = np.array(get_input('сколько лет живут дельфины - не известно. однако не стоит недооценивать их ум')) # Example random input data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(get_input(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mсколько лет живут дельфины - не известно. однако не стоит недооценивать их ум\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# Example random input data\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Add two new columns filled with zeros\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m zeros \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Concatenate along axis 1 (columns)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((input_data, zeros), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "# test sbert\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the ONNX model\n",
    "model_path = \"sbert_onnx.onnx\"  # Replace with your ONNX model path\n",
    "session = ort.InferenceSession(model_path)\n",
    "\n",
    "# Step 2: Get model input details\n",
    "input_name = session.get_inputs()[0].name  # Get the input layer name\n",
    "input_shape = session.get_inputs()[0].shape  # Get the input shape\n",
    "input_type = session.get_inputs()[0].type  # Get the input data type\n",
    "\n",
    "print(f\"Input Name: {input_name}\")\n",
    "print(f\"Input Shape: {input_shape}\") \n",
    "print(f\"Input Type: {input_type}\")\n",
    "\n",
    "input_data = np.array(get_input('сколько лет живут дельфины - не известно. однако не стоит недооценивать их ум')) # Example random input data\n",
    "# Add two new columns filled with zeros\n",
    "zeros = np.zeros((input_data.shape[0], input_shape[1] - len(input_data[0])), dtype=np.int64)\n",
    "# Concatenate along axis 1 (columns)\n",
    "input_data = np.concatenate((input_data, zeros), axis=1)\n",
    "# Step 4: Run the model (perform inference)\n",
    "output_1 = session.run(None, {input_name: input_data})\n",
    "print(torch.tensor(output_1).shape)\n",
    "output_1 = mean_pooling(torch.tensor(output_1[0]), torch.tensor(input_data[2]))\n",
    "print(output_1)\n",
    "\n",
    "input_data = np.array(get_input('сколько лет живут дельфины - никому не известно. однако не стоит недооценивать их ум?')) # Example random input data\n",
    "\n",
    "zeros = np.zeros((input_data.shape[0], input_shape[1] - len(input_data[0])), dtype=np.int64)\n",
    "\n",
    "input_data = np.concatenate((input_data, zeros), axis=1)\n",
    "print(input_data[0])\n",
    "\n",
    "output_2 = session.run(None, {input_name: input_data})[0]\n",
    "output_2 = mean_pooling(torch.tensor(output_2), torch.tensor(input_data[2]))\n",
    "print(output_1)\n",
    "\n",
    "dot_product = np.dot(output_1[0], output_2[0])\n",
    "\n",
    "magnitude_A = np.linalg.norm(output_1[0])\n",
    "magnitude_B = np.linalg.norm(output_2[0])\n",
    "\n",
    "cosine_similarity = dot_product / (magnitude_A * magnitude_B)\n",
    "\n",
    "print(\"Model Output:\", cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101, 117991,    105,   1085,    672,  12269,    177,    102,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101,    113,   8188,   2974,   1114,   1349,  29586,  13318,    121,\n",
      "           1153,   1232,  27668,    689,  51637,    126,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[[ 0.8042, -0.1749, -0.2568,  ...,  0.7997, -0.2365, -0.0497],\n",
      "         [ 0.4390, -0.4258,  0.2302,  ...,  0.4058, -0.9233, -0.2085],\n",
      "         [ 0.5181, -0.3981, -0.1024,  ...,  0.0260, -0.2736, -0.1964],\n",
      "         ...,\n",
      "         [ 0.3521, -0.2190, -0.1988,  ...,  0.6054, -0.7527, -0.0376],\n",
      "         [ 0.4132, -0.1968, -0.1096,  ...,  0.5381, -0.7649, -0.1167],\n",
      "         [ 0.4825, -0.1723, -0.1717,  ...,  0.6319, -0.6655,  0.0641]],\n",
      "\n",
      "        [[ 0.2119, -0.2983, -0.9224,  ..., -0.1061, -0.4942,  0.8632],\n",
      "         [ 0.0524, -0.3483, -1.1260,  ...,  0.0870,  0.5575, -0.4386],\n",
      "         [ 0.2123, -0.5450, -1.3360,  ...,  0.3235,  0.6475, -0.2057],\n",
      "         ...,\n",
      "         [ 0.6332, -0.4581, -1.1514,  ...,  0.0590,  0.0082, -0.3778],\n",
      "         [-0.6489, -0.7999, -1.0493,  ..., -0.5638, -0.0301,  0.3302],\n",
      "         [ 0.3628, -0.4569, -1.2650,  ...,  0.1747,  0.0420,  0.5180]]])\n",
      "tensor([[ 0.4624, -0.3147, -0.1613,  ...,  0.1908, -0.3822, -0.1491],\n",
      "        [ 0.3098, -0.5198, -1.3013,  ..., -0.0418,  0.0242, -0.0041]])\n",
      "0.18307209\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "\n",
    "#Sentences we want sentence embeddings for\n",
    "sentences = ['Дельфин ежу не товарищ!',\n",
    "'В нашем городе есть много красивых парков, где можно гулять со знакомыми.']\n",
    "\n",
    "#Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "model = AutoModel.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "print(model_output[0])\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(sentence_embeddings)\n",
    "dot_product = np.dot(list(sentence_embeddings[0]), list(sentence_embeddings[1]))\n",
    "\n",
    "# Step 2: Compute the magnitudes (Euclidean norms) of A and B\n",
    "magnitude_A = np.linalg.norm(sentence_embeddings[0])\n",
    "magnitude_B = np.linalg.norm(sentence_embeddings[1])\n",
    "\n",
    "# Step 3: Compute cosine similarity\n",
    "cosine_similarity = dot_product / (magnitude_A * magnitude_B)\n",
    "print(cosine_similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
