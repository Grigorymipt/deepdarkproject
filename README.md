# deepdarkproject
Мы (команда Deep Dark Learning) представляем остов решения предложенной партнёром ГПН задачи о построении карты знаний. Постановка задачи оставила некоторую свободу трактовки понятия Knowledge Graph, мы остановились на следующей:

Markdown-cтруктура каждого файла содержит информацию об отношениях между разделами текста. Эта информация может быть представлена в виде графа, вершины которого будут соответствовать тексту разделов, а направленные рёбра - отношению "является подразделом". Его удобно изобразить вертикально, тогда каждому документу будет соответствовать "многоэтажное" дерево, где "этажи" отвечают уровням вложенности разделов, как в генеалогическом дереве они отвечают поколениям. 

Основной же интерес вызывают семантические связи между разделами разных документов - их построение и представляет собой задачу машинного обучения. 

Таким образом pipeline от базы документов до Knowledge Graph приримает чёткий вид:
1. Простой пользовательский интерфейс, принимающий набор файлов markdown
2. Алгоритмический код, преобразующий документы в деревья разделов
3. Модель NLP, определяющая связи между разделами разных документов
4. Простой пользовательскийй интерфейс, отображающий результирующий граф 

## Альтернативный подход

Кроме этого, рассматривался подход, игнорирующий структуру входных файлов. Часто построением Knowledge Graph называется создание графа, в вершинах которого находятся Named Entities - сущности (имена людей, названия компаний, события и т.д.), упоминаемые в базе текстов. Они связаны рёбрами разных "цветов", обозначающие разные смысловые отношения. В таком случае задача распадается на шаги, первый - Named Entity Recognition - определение этих сущностей, для которого успешно используются модели, основанные на архитектуре BERT, такие как [KeyBERT](https://github.com/MaartenGr/KeyBERT). Затем следует relation extraction - выявление отношений между этими сущностями. Для этого, [насколько нам известно](https://ceur-ws.org/Vol-3749/genesy-04.pdf),  лучше всего работают "большие языковы модели" - LLM. Генерируется запрос, содержащий текст и набор сущностей в нём, в котором от модели требуется по сути список рёбер - троек вида (Entity1, Entity2, Relation).    

Мы решили отказаться от этого подхода, поскольку он не принимает во внимание внутреннюю структуру документов. Получающийся Knowledge Graph призван описывать не конкретную базу документов, а абстрактную область знания, о которой эти документы содержат информацию. Связи между разделами одного документа буду восстановлены далеко не всегда. Однако, в случае формальной документации полезные для ответа на запрос к RAG-системе данные часто будут распределены по разделам одного или нескольких похожих документов.  

## Распределение ролей

Эти подзадачи и были разделены между участниками команды:

1) Обработать входные md файлы, переведя их в удобный для нас формат.
2) Разработать пайплайн на python для формализации взаимосвязей между параграфами в разных файлах и выстраивания "горизонтальных" связей.
3) Во время и после разработки модели основного модуля c использованием Python перенести полученные результаты на наш Стек: .NET и реализовать соответствующий модуль.
4) Разработать CRUD(create, read, update, delete) API на .NET для взаимодействия микросервиса с БД.
5) Разработать алгоритм для построения карты знаний по полученному на предыдущих шагах набору вершин и рёбер.
6) Разработать красивый, интерактивный UI, позволяющий в реальном времени отслеживать процессы работы микросервиса.


## Задача NLP

Поставленную задачу нам кажется естественным решать как задачу Semantic Textual Similarity. Она сводится к сравнению пар текстов разделов, в котором языковая модель используется для получения количественной меры их семантической схожести. В таком случае вершины (разделы) будут соединены ребром, если этот показатель превысит некоторую отсечку - гиперпараметр модели.

### Выделение ключевых слов

На раннем этапе возникала мысль о том, что соединить связью имеет смысл те разделы, в которых говорится об одном и том же объекте, часто упоминаемом в корпусе документов (также удобно придать этой связи смысл, указав, какой общий объект есть у этой пары текстов). Для этого предполагалось решить задачу NER для каждого документа с помощью [KeyBERT](https://github.com/MaartenGr/KeyBERT). Это ставило дополнительные задачи, например предобработки русскоязычного текста - стемминг или лемматизацию ([pymorphy2](https://pymorphy2.readthedocs.io/en/stable/)). Тестируя этот подход параллельно с основным, мы обнаружили, что KeyBERT (работающий на мультиязыковой модели за отсутствием специальной для русского) выделяет в качестве ключевых фраз несвязные фрагменты текста, независимо от предобработки или её отсутствия. От этого подхода было решено отказаться. 

### Построение embedding'ов

Идея здесь в том, чтобы построить векторное представление текста (в нашем случае - раздела документа), отображающее его смысл. Для получения этих представлений стандартно используется энкодер модели типа BERT. Она использует две глубокие нейросети, основанные на архитектуре Transformer, чтобы по последовательности токенов сначала получить векторное представление окрестности каждого слова, а затем с его помощью предсказать слово по контексту. Это векторное представление (вывод первой нейросети) несёт информацию о смысле текста, поданного на вход модели, и мера угла между векторами двух текстов в пространстве скрытого слоя будет отражать степень близости их значений.

Для этого использовался ряд предобученных моделей. Стандартная библиотека [Sentence Transformers](https://github.com/UKPLab/sentence-transformers) не содержит модели, обученной для русского языка, но в качестве потенциального baseline была произведена оценка многоязыковой модели paraphrase-multilingual-MiniLM-L12-v2. Позже, использовалась модель [RuBERT](https://huggingface.co/DeepPavlov/rubert-base-cased-sentence) от DeepPavlov, обученная специально для русского языка. Оценка производилась на размеченном вручную игрушечном датасете. Она показывает ROC-AUC=0.71, и будет являться baseline-реализацией. 

!["ROC"](./assets/dp_roc.png)

Пути дальнейшей разработки - повторение тестов на [SOTA-модели](https://habr.com/ru/companies/sberdevices/articles/527576/) извлечения текстовых эмбеддингов для русского языка [SBERT](https://huggingface.co/ai-forever/sbert_large_nlu_ru), потенциально - файнтюнинг модели на [корпусе](https://github.com/irlcode/RusLawOD) официальных документов на русском языке. 

## Обработка входных данных, отрисовка карты знаний

На данном этапе был реализован класс на языке C# для визуализации карты знаний в виде планарного графа. Для этого была использована библиотека OxyPlot. Для распределения вершин на изображении для начала был выбран способ со случайным местоположением. В дальнейшем планируется использование более продвинутых алгоритмов, таких как  Toggle the table of contents

Force-directed graph drawing. Также, для возможности интерактивного взаимодействияпользователя с картой знаний, будет разработан и проработан UI с логикой на JavaScript.


## Заключение
Наша команда из пяти человек успешно продвигается в разработке микросервиса для обработки Markdown (MD) файлов и создания по ним карты знаний. Мы используем Agile-подход, что позволяет гибко реагировать на изменения в требованиях и активно взаимодействовать в процессе разработки. За время работы над проектом была выполнена следующая работа:

###Архитектура микросервиса
Мы разработали архитектуру микросервиса, обеспечивающую модульность, масштабируемость и лёгкость интеграции. Архитектура позволяет отдельным компонентам (таким как парсеры, алгоритмы обработки данных, базы данных и интерфейсы) взаимодействовать через API, что упрощает тестирование и последующие улучшения системы.

### Алгоритм обработки входных данных
Создан основной алгоритм обработки входных данных, который принимает MD файлы и переводит их в структуру, пригодную для анализа и визуализации. Этот алгоритм включает в себя этапы чтения и парсинга текстовой информации, а также дальнейшую её подготовку для создания карты знаний.

### Токенизатор
Мы использовали токенизатор, который разбивает текст на логические единицы (токены). Токенизация — это важный этап для дальнейшего анализа данных, а также для точного представления связей между объектами в карте знаний.

### Эмбеддер
Эмбеддер преобразует текстовые токены в числовые представления, подходящие для вычислительной обработки. Это позволяет нам оценивать семантическую близость между разными частями текста, чтобы более точно строить карту знаний и выявлять скрытые взаимосвязи.

### Карта знаний в формате SVG
На основе обработанных данных реализована генерация карты знаний в формате SVG. Эта карта отображает связи между различными концепциями и элементами Markdown файлов.

### HTML страницы с UI
Мы разработали HTML страницы с пользовательским интерфейсом (UI), которые позволяют пользователям взаимодействовать с системой через браузер. UI обеспечивает просмотр карты знаний, управление настройками отображения и поиск по данным. Мы уделили внимание удобству использования и производительности интерфейса.

### Разработка базы данных
На текущий момент активно ведётся разработка базы данных, которая будет хранить результаты обработки файлов, метаданные о связях между элементами, а также промежуточные данные для ускорения дальнейшей обработки. База данных сыграет ключевую роль в масштабируемости и эффективности работы системы, особенно при увеличении объёма данных.

Проект продолжается в соответствии с планом, и мы активно двигаемся к достижению следующих этапов разработки, включая финализацию базы данных и интеграцию всех компонентов в единую систему.

Наш github: https://github.com/Grigorymipt/deepdarkproject/blob/master/DeepDarkService
Наш tg: https://t.me/+IznLyKD_1VUxODNi
